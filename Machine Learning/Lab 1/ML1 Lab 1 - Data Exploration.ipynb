{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "The following exercise takes you through using Jupyter to explore some data, and deriving your own algorithm for classifying it. At the end it includes a simple (and largly useless in the real world) machine learning algorithm for you to try.\n",
    "\n",
    "As an introduction to Jupyter this is a `workbook`, which contains a sequence of `blocks`. This is a `markdown block`, which contains documentation. There are also `code blocks`, which contain code and show the output of running the code below the block. All blocks are editable - you can double click on this to edit it for instance, then hit `run` (icon above with an arrow pointing right into a line) to return it to its current `formatted` state (you don't need to double click on code blocks as they only have one state).\n",
    "\n",
    "Be aware that this workbook requires that one other file exist in the same directory:\n",
    " * Skin_NonSkin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking and Submission\n",
    "\n",
    "While this lab exercise is formative a mark scheme is still included, for a total of 10 marks. Every place you have to add code is indicated by\n",
    "\n",
    "`# **************************************************************** 2 marks`\n",
    "\n",
    "with instructions above the code block.\n",
    "\n",
    "The suggested deadline is 2021-10-22 20:00 BST; you can use the auto marker to get a mark whenever you want. The workbook you submit must be an .ipynb file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`. Remember to save your work regularly (`Save and checkpoint` in the `File` menu, the icon of a floppy disk, or `Ctrl-S`); the version you submit should have all code blocks showing the results (if any) of execution below them.\n",
    "\n",
    "You must comply with the universities plagiarism guidelines: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The below code block is simply preparation:\n",
    "\n",
    "__%matplotlib inline__: Tells Jupyter to display the graphs generated by matplotlib in the workbook.\n",
    "\n",
    "__import numpy__: Loads 'numerical python', which provides the array objects we store data in.\n",
    "\n",
    "__import matplotlib.pyplot as plt__: Loads an easy to use matplotlib interface, for plotting graphs.\n",
    "\n",
    "Run the below code block by clicking the run icon (an arrow pointing at a line) in the icon line above or typing `ctrl-enter`. The `In[ ]:` next to it will change to `In[1]:` to indicate it was the first code block run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Step 1 of any data exploration task is to load the data. \n",
    "\n",
    "For this part of the exercise we will be classifying skin. Specifically, when you take a photograph some pixels are of peoples skin, others are not (e.g. they might be part of a mug of coffee). The machine learning task is to learn a classifier that identifies which pixels are skin and which are not given many exemplars. In a real application a  digital camera may use this as a first step in finding peoples faces, so that is can focus on them or avoid taking photos when people blink.\n",
    "\n",
    "The inputs are: \n",
    " * r - How much red is in the pixel, an integer in [0, 255].\n",
    " * g - How much green is in the pixel, an integer in [0, 255].\n",
    " * b - How much blue is in the pixel, an integer in [0, 255].\n",
    "\n",
    "The output is:\n",
    " * 0 - It is a background pixel.\n",
    " * 1 - It is skin.\n",
    "\n",
    "\n",
    "You need the file `Skin_NonSkin.txt` in the same directory as this workbook for the below code block to work - it should be avaliable on moodle. Alternatively, this data set was obtained from http://archive.ics.uci.edu/ml/datasets/Skin+Segmentation Remember to run it using `ctrl-enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for reading...\n",
    "f = open('Skin_NonSkin.txt', 'r')\n",
    "\n",
    "# Each line in the file is an exemplar, unless empty...\n",
    "data = []\n",
    "for line in f.readlines():\n",
    "    parts = [int(v) for v in line.split()]\n",
    "    if len(parts)==4:\n",
    "        data.append(parts)\n",
    "\n",
    "# Close file...\n",
    "f.close()\n",
    "\n",
    "\n",
    "# The data as loaded above is not very conveniant for processing - convert to numpy arrays.\n",
    "# Note that the data in the file uses different conventions to those described above, so\n",
    "# there is some cleanup to make it more conveniant...\n",
    "data = numpy.array(data)\n",
    "\n",
    "in_r = data[:,2]\n",
    "in_g = data[:,1]\n",
    "in_b = data[:,0]\n",
    "\n",
    "out = data[:,3]\n",
    "out[out==2] = 0\n",
    "\n",
    "\n",
    "# Print out how many exemplars were loaded and some very basic statistics...\n",
    "print('Number of exemplars:', out.shape[0])\n",
    "print('Number of background pixels =', (out==0).sum())\n",
    "print('Number of skin pixels =', (out==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "\n",
    "For reasons which will be covered in a later lecture you must always have _independent_ test and train data sets - in this case we will have one data set for training, and one for testing. The below code performs this split, putting half of each kind into each set.\n",
    "\n",
    "(Sometimes data sets are provided with a train/test split already built in, which can be good for making sure everyone has comparable results. The 50:50 split is arbitrary - many other choices can be made, as can different choices regarding numbers of training and testing data sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates the indices of exemplars for each kind...\n",
    "index_bg = numpy.nonzero(out==0)[0] # bg is short for background.\n",
    "index_skin = numpy.nonzero(out==1)[0]\n",
    "\n",
    "\n",
    "# Split to get indices for each set...\n",
    "half_bg = index_bg.shape[0] // 2\n",
    "half_skin = index_skin.shape[0] // 2\n",
    "\n",
    "index_train = numpy.concatenate((index_bg[:half_bg],index_skin[:half_skin]))\n",
    "index_test = numpy.concatenate((index_bg[half_bg:],index_skin[half_skin:]))\n",
    "\n",
    "\n",
    "# Now use the indices to generate the vectors of data for each case...\n",
    "train_in_r = in_r[index_train]\n",
    "train_in_g = in_g[index_train]\n",
    "train_in_b = in_b[index_train]\n",
    "train_out = out[index_train]\n",
    "\n",
    "test_in_r = in_r[index_test]\n",
    "test_in_g = in_g[index_test]\n",
    "test_in_b = in_b[index_test]\n",
    "test_out = out[index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Statistics\n",
    "\n",
    "Given a new data set the first thing you may want to look at is its statistics, such as mean and standard deviation (for the training set only). The below code prints out the global statistics, your task is to also print out the statistics for background and skin independently. You may get just the members of a vector that correspond to the background by, e.g. `train_in_r[train_out==0]`.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Global:')\n",
    "print('  Red: mean =', numpy.mean(train_in_r), '| sd =', numpy.std(train_in_r))\n",
    "print('  Green: mean =', numpy.mean(train_in_g), '| sd =', numpy.std(train_in_g))\n",
    "print('  Blue: mean =', numpy.mean(train_in_b), '| sd =', numpy.std(train_in_b))\n",
    "print('')\n",
    "\n",
    "# **************************************************************** 1 mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Simplest Classifier\n",
    "\n",
    "Probably the simplest classifier is a single if statement based on a test, e.g.\n",
    "```\n",
    "if feature < 12:\n",
    "    return 1\n",
    "else:\n",
    "    return 0\n",
    "```\n",
    "\n",
    "For continuous features such as colour `less than` or `greater than` tests are a good choice. Fill in the below function with a test based on your intuition from looking at the basic statistics you calculated above (expect to obtain around 70-85% correct).\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a classification function...\n",
    "def is_skin1(r, g, b):\n",
    "    # **************************************************************** 1 mark\n",
    "    pass\n",
    "\n",
    "# Code to test classificatuion performance on the test set...\n",
    "estimate = numpy.array([is_skin1(test_in_r[i], test_in_g[i], test_in_b[i]) for i in range(test_in_r.shape[0])])\n",
    "\n",
    "correct = (estimate==test_out).sum()\n",
    "percentage = 100.0 * correct / float(test_in_r.shape[0])\n",
    "\n",
    "print('Got', percentage, '% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisation for a Better Guess\n",
    "\n",
    "Looking at just the mean and standard deviation is no way to make a guess, even though it does surprisingly well in this case. A better choice is to look at colour coded scatter plots of the data. As you can only look at two dimensions at a time (your screen is flat) and we have three features a good approach is to plot all combinations of two variables, e.g. red vs green, red vs blue and green vs blue. You may want to set the marker parameter of scatter(...) to improve visibility, for instance marker='+' gets you small crosses, marker='.' gets dots.\n",
    "\n",
    "Finish the below code, where red vs green has already been provided.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Red vs Green:')\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(train_in_r[train_out==0], train_in_g[train_out==0], c='b') # c='b' means blue, for background.\n",
    "plt.scatter(train_in_r[train_out==1], train_in_g[train_out==1], c='r') # c='r' means red, for, errr, skin.\n",
    "plt.show()\n",
    "\n",
    "# **************************************************************** 1 mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Better Classifier\n",
    "\n",
    "Instead of applying the split (`if` statement) to a feature directly we can calculate some function of several features. For instance, we could sum two features\n",
    "\n",
    "```\n",
    "if g+b > 42:\n",
    "    return 1\n",
    "else:\n",
    "    return 0\n",
    "```\n",
    "\n",
    "Additionally, instead of a simple split we could consider distance from an ideal value,\n",
    "\n",
    "```\n",
    "if numpy.fabs((g+b) - 22) < 8:\n",
    "    return 1\n",
    "else:\n",
    "    return 0\n",
    "```\n",
    "\n",
    "In the above `22` is the ideal value and `8` the threshold, so the above considers any pixel where `g+b` is between `22-8` and `22+8` to be skin. Note that `22` and `8` can be derived from the mean and variance of the skin respectively (for some multipliar of the variance). A good choice of function and ideal value/threshold should seperate the two classes.\n",
    "\n",
    "Below you need to write code that combines these two ideas:\n",
    " 1. Look at the graphs and select a function of the original features that will improve seperation.\n",
    " 2. Print out simple statistcs for the new function.\n",
    " 3. Write a function based on the simple statistcs (expect to get 80-95% correct).\n",
    "\n",
    "__(3 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training features to calculate a new 'function' of the data,\n",
    "# that is a single value per exemplar that is helpful for classifying skin\n",
    "# assuming the the application of a less than/greater than test.\n",
    "# (train_tform will be a new 1D array, containg one value per exemplar,\n",
    "# e.g. 2 * train_in_b + train_in_g)\n",
    "\n",
    "# **************************************************************** 2 marks\n",
    "train_tform =\n",
    "\n",
    "\n",
    "# Print out simple statistics for your transformation of the data...\n",
    "\n",
    "# **************************************************************** 0 marks :-P\n",
    "\n",
    "\n",
    "# Write a new and improved classifier - this will be the same as the transform\n",
    "# above, but with the less than/greater than check and as an actual function...\n",
    "def is_skin2(r, g, b):\n",
    "    # **************************************************************** 1 mark\n",
    "    pass\n",
    "\n",
    "\n",
    "# Code to test classification performance on the test set...\n",
    "estimate = numpy.array([is_skin2(test_in_r[i], test_in_g[i], test_in_b[i]) for i in range(test_in_r.shape[0])])\n",
    "\n",
    "correct = (estimate==test_out).sum()\n",
    "percentage = 100.0 * correct / float(test_in_r.shape[0])\n",
    "\n",
    "print('Got', percentage, '% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning\n",
    "\n",
    "You are yet to do any actually machine learning - instead you have been manually proposing and programming classifiers, then measuring their performance. Instead, we can get the computer to try lots of different classifiers and select the best one. This is where the train/test split becomes important. We choose the classifier that gets the best score on the train set, but then verify it's performance on the test set.\n",
    "\n",
    "To do this you need to perform four steps:\n",
    " 1. Code a function that classifies your data based on some parameters.\n",
    " 2. Code a function that scores your test data based on the same parameters.\n",
    " 3. Generate lots of different parameters, and select the set that have the best performance.\n",
    " 4. Score the parameters on the test set.\n",
    "\n",
    "This is what you need to code below.\n",
    "\n",
    "\n",
    "Unlike the above where you coded the function directly, now you need to code a function of some parameters and then find the best parameters. A very common function is a linear sum of the features, specifically `r*sr + g*sg + b*sb` where `sr`, `sg` and `sb` are the parameters to be learned. We can then consider distance from an ideal value, introducing two further parameters, `ideal` for the ideal value and `threshold` to decide how close to the ideal value you have to be to be considered as skin.\n",
    "\n",
    "Be careful how many parameter combinations you try - no effort has been made to be efficient, so it could take a long time to run! You will probably want to print out some kind of progress report. You should be able to obtain over 98%, though you won't lose marks if you don't. Note however that if your getting less than 80% something is definitely wrong!\n",
    "\n",
    "\n",
    "_Note 1_: Remember the relationship above, you don't want to loop over values for the ideal value, and will want to look over multiples of the standard deviation for the threshold.\n",
    "\n",
    "_Note 2_: You may find the function numpy.linspace(...) useful for generating a selection of parameters to try.\n",
    "\n",
    "_Note 3_: This approach of simply trying every combination is called `brute force` - it works on small problems like this, but does not scale to problems of even moderate size. This is not to say it is never used - there are plenty of small problems in the world for which it is the right approach, but most computer scientists would be embarrassed to admit they used it.\n",
    "\n",
    "_Note 4_: You will probably notice that your performance on the training set is better than the test set. This is called overfitting, and discussed in a later lecture.\n",
    "\n",
    "__(4 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code a function to classify an input colour (r,g b) which is driven by the 5 parameters above...\n",
    "def is_skin3(r, g, b, sr, sg, sb, ideal, threshold):\n",
    "    # **************************************************************** 1 marks\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Fill in this function that scores the train data and return the percentage of right values for the train data...\n",
    "def score(sr, sg, sb, ideal, threshold):\n",
    "    # **************************************************************** 1 mark\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a (nested) loop that tries out lots of different parameters.\n",
    "# A sugestion for the outer level of the loop has been provided for you.\n",
    "\n",
    "def optimise():\n",
    "    best_score = -1.0\n",
    "    best_sr = None\n",
    "    best_sg = None\n",
    "    best_sb = None\n",
    "    best_ideal = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for sr in [-1.0, -0.5, 0.0, 0.5, 1.0]:\n",
    "        print('Trying sr =', sr)\n",
    "        # **************************************************************** 2 marks\n",
    "    \n",
    "    return best_sr, best_sg, best_sb, best_ideal, best_threshold\n",
    "\n",
    "\n",
    "# Run above...\n",
    "best_sr, best_sg, best_sb, best_ideal, best_threshold = optimise()\n",
    "\n",
    "# This code will judge the classifier your computer has learned...\n",
    "estimate = numpy.array([is_skin3(test_in_r[i], test_in_g[i], test_in_b[i], best_sr, best_sg, best_sb, best_ideal, best_threshold) for i in range(test_in_r.shape[0])])\n",
    "\n",
    "correct = (estimate==test_out).sum()\n",
    "percentage = 100.0 * correct / float(test_in_r.shape[0])\n",
    "\n",
    "print('Final classifier:')\n",
    "print('  sr =', best_sr)\n",
    "print('  sg =', best_sg)\n",
    "print('  sb =', best_sb)\n",
    "print('  ideal =', best_ideal)\n",
    "print('  threshold =', best_threshold)\n",
    "print('')\n",
    "print('  Got', percentage, '% correct')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}